% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adamReg.R
\name{adam}
\alias{adam}
\title{Adaptive moment estimation for optimization of logistic regression}
\usage{
adam(
  X,
  Y,
  batch_size = 32,
  alpha = 0.001,
  beta_decay = c(0.9, 0.999),
  epsilon = 1e-08,
  maxit = 1000,
  tol = 1e-04,
  check_conv = TRUE
)
}
\arguments{
\item{X}{A matrix of features.}

\item{Y}{The response vector.}

\item{batch_size}{Size of mini-batches used in the gradient descent algorithm.}

\item{alpha}{The learning rate (step size).}

\item{beta_decay}{The exponential decay rates for moment estimates. Must be in [0, 1).}

\item{epsilon}{Small value to avoid division by zero. Must be positive.}

\item{maxit}{Maximum number of iterations.}

\item{tol}{Tolerance for convergence.}

\item{check_conv}{When TRUE, reports the number of iterations needed for convergence.}
}
\value{
A numeric vector of estimated coefficients.
}
\description{
Estimates the coefficients of logistic regression, using an algorithm
for first-order gradient-based optimization of stochastic objective functions,
based on adaptive estimates of lower-order moments.
From Kingma, D. P., & Ba, J. (2014).
Adam: A Method for Stochastic Optimization.
https://arxiv.org/abs/1412.6980.
}
\details{
Default values for parameters based on suggestions from Kingma and Ba (2014).
}

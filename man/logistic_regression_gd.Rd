% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adamReg.R
\name{logistic_regression_gd}
\alias{logistic_regression_gd}
\title{Perform gradient descent optimization for logistic regression.}
\usage{
logistic_regression_gd(
  X,
  Y,
  alpha = 0.01,
  penalty = "none",
  lambda = 0,
  maxit = 1000,
  tol = 1e-06,
  check_conv = TRUE
)
}
\arguments{
\item{X}{A matrix of features.}

\item{Y}{The response vector, binary.}

\item{alpha}{The learning rate (step size), fixed.}

\item{penalty}{The penalty (L1, L2) for the loss function.}

\item{lambda}{The numeric value of the penalty.}

\item{maxit}{The maximum number of iterations.}

\item{tol}{The tolerance for convergence.}

\item{check_conv}{When TRUE, reports the number of iterations needed for convergence.}
}
\value{
A numeric value.
}
\description{
Implements the gradient descent algorithm for logistic regression with a fixed learning rate.
}
\details{
The gradient without penalties is: \deqn{-X^T(Y-\mu)}

The gradient with L1 penalty ("LASSO") is: \deqn{-X^T(Y-\mu) + \lambda\sum_{i=1}^n{|\beta|}}

The gradient with L2 penalty ("Ridge") is: \deqn{-X^T(Y-\mu) + \frac{\lambda}{2}\sum_{i=1}^n{\beta^2}}

where \eqn{\mu = \frac{e^{X^T\beta}}{1+e^{X^T\beta}}}.

The update step is given by \deqn{\theta_{t+1} = \theta_t - \alpha \nabla L(\theta_t)}
}

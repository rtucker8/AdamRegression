% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adamReg.R
\name{cross_validate_adam}
\alias{cross_validate_adam}
\title{Cross-validation of adam for optimal lambda}
\usage{
cross_validate_adam(
  k = 5,
  X,
  Y,
  penalty,
  batch_size = 32,
  alpha = 0.001,
  beta_decay = c(0.9, 0.999),
  epsilon = 1e-07,
  maxit = 10000,
  tol = 0.001
)
}
\arguments{
\item{k}{Number of folds used.}

\item{X}{A matrix of features.}

\item{Y}{The response vector.}

\item{penalty}{The penalty (L1, L2) for the loss function.}

\item{batch_size}{Size of mini-batches used in the gradient descent algorithm.}

\item{alpha}{The learning rate (step size).}

\item{beta_decay}{The exponential decay rates for moment estimates. Must be in [0, 1).}

\item{epsilon}{Small value to avoid division by zero. Must be positive.}

\item{maxit}{Maximum number of iterations.}

\item{tol}{Tolerance for convergence.}
}
\value{
An optimal lambda value alongside a plot of all lambda values explored.
}
\description{
Cross-validation to identify the optimal lambda to be used in adam optimization for estimating
the coefficients in logistic regression. See \link{adam}.
}
\details{
From Kingma, D. P., & Ba, J. (2014).
Adam: A Method for Stochastic Optimization.
https://arxiv.org/abs/1412.6980.
}
